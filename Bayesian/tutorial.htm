<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Think Bayesian</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><h1 id="bayes-theorem">Bayes Theorem</h1>

<p><script type="math/tex" id="MathJax-Element-1">\theta -</script>  Parameters <br>
<script type="math/tex" id="MathJax-Element-2">X - </script> Observations</p>

<p><img src="https://lh3.googleusercontent.com/xL_wgSlIv21xg6F3xD7wzyld_Pu6sRCuQYScxuBJ7w5Y5UchVeZ0vE3HiqUkrHUc-xCLyLB0VoA=s0" alt="enter image description here" title="Screen Shot 2017-11-14 at 9.21.48 PM.png"></p>



<h3 id="probability-rules">Probability rules:</h3>

<ul>
<li><strong>Sum rules:</strong> Marginalization from joint distribution <script type="math/tex; mode=display" id="MathJax-Element-3">p(X) = \int_{-\infty}^\infty p(X,Y) dY</script></li>
<li><strong>Chain rules:</strong> <script type="math/tex; mode=display" id="MathJax-Element-4">
  P(X, Y) = P(X|Y)P(Y)=P(Y|X)P(X)\\
  P(X,Y,Z)=P(X|Y,Z)P(Y,Z)=P(X|Y,Z)P(Y|Z)P(Z)\\
  P(X_1,\ldots,X_N)=\prod_{i=1}^NP(X_i|X_1,\ldots,X_{i-1})
  </script></li>
</ul>



<h3 id="point-estimation-frequentist-vs-bayesian">Point Estimation (Frequentist vs. Bayesian)</h3>

<p>Rather than estimate the entire distribution <script type="math/tex" id="MathJax-Element-5">p(θ|x)</script>, sometimes it is sufficient to find a single ‘good’ value for <script type="math/tex" id="MathJax-Element-6">θ</script>. We call this a point estimate.</p>

<ul>
<li>Frequentist thinks parameters <script type="math/tex" id="MathJax-Element-7">\theta</script> are fixed, data <script type="math/tex" id="MathJax-Element-8">X</script> are random.  <br>
Maximum Likelihood Estimation: <script type="math/tex; mode=display" id="MathJax-Element-9">\hat{\theta}=\arg\max_{\theta} P(X|\theta) </script></li>
<li>Bayesian thinks parameters <script type="math/tex" id="MathJax-Element-10">\theta</script> are random, data <script type="math/tex" id="MathJax-Element-11">X</script> are fixed. <br>
Maximum Aposteriori Estimation (MAP) <script type="math/tex; mode=display" id="MathJax-Element-12">\hat{\theta}=\arg\max_{\theta} P(\theta|X) </script>  <br>
<ul><li>MAP estimation is not invariant to non-linear transformations of <script type="math/tex" id="MathJax-Element-13">θ</script>. E.g. A non-linear transformation  <script type="math/tex" id="MathJax-Element-14">\theta^\prime=g(\theta)</script>, to <script type="math/tex" id="MathJax-Element-15">\theta</script>  can shift the posterior mode in such a way that <script type="math/tex" id="MathJax-Element-16">g^{-1}(\mathrm{mode}[\theta^\prime]) \neq \mathrm{mode}[\theta]</script>.</li>
<li>MAP estimate may not be typical of the posterior. </li></ul></li>
</ul>



<h2 id="bayesian-netwok-graphical-model">Bayesian Netwok (Graphical Model)</h2>

<p><img src="https://lh3.googleusercontent.com/EbDwfsTX7a-plaotkXFSJ4kyDZ-2BhE6rgpUYRjWaC1ThN_Emp-chhKVhP5-PFUKhEaMqSpHR4M=s400" alt="enter image description here" title="Screen Shot 2017-11-14 at 9.48.06 PM.png"></p>

<ul>
<li>Nodes are random variables</li>
<li>Edges indicates dependence (e.g. Grass is wet depends on both sprinkler or rain, and whether sprinkler is on or off depends on rain)</li>
<li>Observed variables are shaded nodes; unshaded nodes are hidden</li>
<li>Plated denote replicated structure</li>
</ul>

<p>Joint probability over all the variables in the above model is given by: <br>
<img src="https://lh3.googleusercontent.com/TN2FS7xe1saG8IZqYbqUHwC78CZTw-kZGztDlt5oyT1efKU8vYJwMVP4SXTwIbVzosPdqg_RHzY=s400" alt="enter image description here" title="Screen Shot 2017-11-14 at 9.52.05 PM.png"></p>

<p><strong>Example 1:</strong> <br>
<img src="https://lh3.googleusercontent.com/nStGvG60Q40IsXdBcoWiYRkIYG4jX04daZ_io4fhLNiaOZ5ycYw-K9In8eqyzcet45fuPANQvQE=s300" alt="enter image description here" title="Screen Shot 2017-11-14 at 9.54.38 PM.png"> Here,  <script type="math/tex" id="MathJax-Element-17">P(S,R,G) = P(G|S, R) P(S|R) P(R)</script></p>

<p><strong>Example 2: Naive Bayes Classifer</strong> <br>
<img src="https://lh3.googleusercontent.com/-8YmaTwGT5Nw/WgvYHhwHclI/AAAAAAAAADs/xit9iNys-K0ekgzOXBlSouASqQdQNoaKwCLcBGAs/s300/Screen+Shot+2017-11-14+at+9.58.55+PM.png" alt="enter image description here" title="Screen Shot 2017-11-14 at 9.58.55 PM.png"> </p>

<p>Joint Probability <script type="math/tex" id="MathJax-Element-18">P(c, f_1,\ldots,f_N)=P(c)\prod_{i=1}^N P(f_i|C)</script></p>

<p>In plate notation, the figure above can be shortened as follows:</p>

<p><img src="https://lh3.googleusercontent.com/-34usjTZw2_o/WgvYCVVgoxI/AAAAAAAAADk/nz0kDVbkUUUNzJtWj646AvBG0nZl_DJfwCLcBGAs/s300/Screen+Shot+2017-11-14+at+9.59.02+PM.png" alt="enter image description here" title="Screen Shot 2017-11-14 at 9.59.02 PM.png"></p>



<h1 id="conjugate-prior">Conjugate Prior</h1>

<ul>
<li>MIT <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf">lecture note</a> has a good section on conjugate prior</li>
</ul>

<p>Point estimation is useful for many applications, however true goal in Bayesian analysis is often to find the full posterior <script type="math/tex" id="MathJax-Element-19">p(\theta|X)</script>. In most cases, it is difficult to calculate the denominator <script type="math/tex" id="MathJax-Element-20">p(X)=\int p(X|\theta)p(\theta)</script>. One approach to circumventing the integral is to use conjugate priors. Here the idea is, if we choose the ‘right’ prior for a particular likelihood function, then we can compute the posterior without worrying about the integral.</p>

<p>Formally, a prior  <script type="math/tex" id="MathJax-Element-21">p(\theta)</script> is <strong>conjugate to the likelihood <script type="math/tex" id="MathJax-Element-22">p(X|\theta)</script></strong>, if the prior  <script type="math/tex" id="MathJax-Element-23">p(\theta)</script> and the posterior  <script type="math/tex" id="MathJax-Element-24">p(\theta|x)</script> are from the same family of distribution.</p>

<p>Examples:</p>

<ul>
<li>Beta distribution is conjugate to Bernoulli likelihood. <a href="http://varianceexplained.org/statistics/beta_distribution_and_baseball/">Here</a> is a good example of this for baseball batting average calculation.</li>
<li>Dirichlet distribution is conjugate to Multinomial likelihood (e.g. application in LDA)</li>
</ul>



<h1 id="variational-inference">Variational Inference</h1>

<ul>
<li>very intuitive explanation in this <a href="http://blog.evjang.com/2016/08/variational-bayes.html">blog</a></li>
</ul>



<h1 id="common-probability-distributions">Common Probability Distributions</h1>



<h2 id="gamma-distribution">Gamma Distribution</h2>

<p><script type="math/tex; mode=display" id="MathJax-Element-25">
p(\gamma|a,b) = \dfrac{b^a}{\Gamma(a)}\gamma ^{a-1}e^{-b\gamma}
</script> <br>
Here,</p>

<ul>
<li><script type="math/tex" id="MathJax-Element-26"> \gamma, a, b > 0</script></li>
<li>support of Gamma distribution is <script type="math/tex" id="MathJax-Element-27">[0, \infty)</script></li>
<li><script type="math/tex" id="MathJax-Element-28">\mathbb{E}[\gamma]=\frac{a}{b}</script></li>
<li><script type="math/tex" id="MathJax-Element-29">\text{Var}[\gamma]=\frac{a}{b^2}</script></li>
</ul>

<p><img src="https://lh3.googleusercontent.com/-qHr1Hzm5u_gI3sS7zT03VfxJW2_50-RqYoLszvmQyLn02oiChCnc8U-JPq3ZADq0TX4iGwXaug=s400" alt="enter image description here" title="Screenshot 2017-11-22 01.46.31.png"></p>

<p><strong>Example:</strong> Suppose I ran 5km <script type="math/tex" id="MathJax-Element-30">\pm</script> 100 m every day, i.e. mean 5km with std 100m. We can model this as Gamma distribution. We can also use Gaussian - however, that means we can run negative distance.</p>

<p><img src="https://lh3.googleusercontent.com/--Q8IPSsVSpw/WhVIE29Nl-I/AAAAAAAAAE0/sJu5DdSPoVUc0QzU3CPJEEeserY7Kq1MgCLcBGAs/s400/Screenshot+2017-11-22+01.48.32.png" alt="enter image description here" title="Screenshot 2017-11-22 01.48.32.png"></p>



<h2 id="beta-distribution">Beta Distribution</h2>

<p><script type="math/tex; mode=display" id="MathJax-Element-31">
p(x|a,b)=\dfrac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}
</script> <br>
Here, <br>
 - a, b &gt; 0 <br>
 - support of beta distribution is [0,1], i.e. <script type="math/tex" id="MathJax-Element-32">x\in [0,1]</script> <br>
 - <script type="math/tex" id="MathJax-Element-33">\mathbb{E}[x]=\frac{a}{a+b}</script> <br>
 - <script type="math/tex" id="MathJax-Element-34">\text{Var}[x]=\frac{ab}{(a+b)^2(a+b-1)}</script></p>

<p><img src="https://lh3.googleusercontent.com/-bYZ8S1hI0Sg/WhVKBX3iKZI/AAAAAAAAAFg/d1Y9XYKJAFwUCQD7sQeSFrqY_5yGHnDPACLcBGAs/s400/Screenshot+2017-11-22+01.57.03.png" alt="enter image description here" title="Screenshot 2017-11-22 01.57.03.png"></p>

<p><strong>Example:</strong> Baseball batting average (its a number between 0 and 1). e.g.<script type="math/tex" id="MathJax-Element-35"> 0.27\pm 0.1</script></p></div></body>
</html>